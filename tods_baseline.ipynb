{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helper import *\n",
    "sys.path.insert(0,'tods') \n",
    "import plotter as pltr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "window_size = 1000  # length of slide window in days\n",
    "step_size = int(0.1 * window_size)\n",
    "detector = 'LSTM'\n",
    "threshold_opt = 'hybrid'\n",
    "dataset = 'SMD'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1. Load Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if dataset == 'SMD':\n",
    "    GROUP = 1\n",
    "    ENTITY = 2\n",
    "    SMD_BASE_PATH = 'Dataset/SMD'\n",
    "\n",
    "    X_train = load_data('%s/train/machine-%d-%d.txt' % (SMD_BASE_PATH, GROUP, ENTITY), header=False)\n",
    "    X_train.columns = ['m%d' % i for i in range(X_train.shape[1])]\n",
    "    X_train.index = pd.date_range('2021/03/02', '2021/03/21', periods=X_train.shape[0])\n",
    "    X_train.index.name = 'timestamp'\n",
    "\n",
    "    X_test = load_data('%s/test/machine-%d-%d.txt' % (SMD_BASE_PATH, GROUP, ENTITY), header=False)\n",
    "    X_test.columns = ['m%d' % i for i in range(X_test.shape[1])]\n",
    "    X_test.index = pd.date_range('2021/03/21', '2021/4/8', periods=X_test.shape[0])\n",
    "    X_test.index.name = 'timestamp'\n",
    "\n",
    "    y_true = pd.read_csv('Dataset/SMD/test_label/machine-%d-%d.txt' % (GROUP, ENTITY), header=None)\n",
    "    y_true.columns = ['label']\n",
    "    y_true.index = X_test.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'CTF':\n",
    "    import pickle as pkl\n",
    "\n",
    "    ENTITY = 0\n",
    "    CTF_BASE_PATH = 'Dataset/CTF/processed'\n",
    "\n",
    "    X_train = load_data('%s/train/%d.csv' % (CTF_BASE_PATH, ENTITY)).iloc[:-1]\n",
    "    X_train.columns = ['m%d' % i for i in range(X_train.shape[1])]\n",
    "    X_train.index = pd.date_range(start='2020/04/18', freq='30s', periods=X_train.shape[0])\n",
    "    X_train.index.name = 'timestamp'\n",
    "\n",
    "    X_test = load_data('%s/test/%d.csv' % (CTF_BASE_PATH, ENTITY)).iloc[:-1]\n",
    "    X_test.columns = ['m%d' % i for i in range(X_test.shape[1])]\n",
    "    X_test.index = pd.date_range(start='2020/04/23', freq='30s', periods=X_test.shape[0])\n",
    "    X_test.index.name = 'timestamp'\n",
    "\n",
    "    with open('Dataset/CTF/label_result/%d.pkl' % ENTITY, 'rb') as f:\n",
    "        y = pd.DataFrame(pkl.load(f), columns=['label'])\n",
    "        y.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Shape:', X_train.shape)\n",
    "print('Test Shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first dimension\n",
    "pltr.plot_data(pd.concat([X_train, X_test], axis=0), 'm0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first dimension\n",
    "pltr.plot_data(X_test, 'm0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_true = y_true.to_numpy()\n",
    "print(\"window size:\", window_size)"
   ]
  },
  {
   "source": [
    "## Step 3. Anomaly Detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if detector == 'DeepLog':\n",
    "    from tods.sk_interface.detection_algorithm.DeepLog_skinterface import DeepLogSKI\n",
    "\n",
    "    transformer = DeepLogSKI(window_size=window_size, features=X_train.shape[1], validation_size=0.3, hidden_size=4, preprocessing=False, verbose=1, batch_size=32, epochs=5)\n",
    "    transformer.fit(X_train)\n",
    "elif detector == 'LSTM':\n",
    "    from tods.sk_interface.detection_algorithm.LSTMODetector_skinterface import LSTMODetectorSKI\n",
    "\n",
    "    transformer = LSTMODetectorSKI(window_size=window_size, step_size=step_size, feature_dim=X_train.shape[1], hidden_dim=4, batch_size=32, epochs=5)\n",
    "    transformer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for primitive in transformer.primitives:\n",
    "    pred_score, relative_error_left_inds, relative_error_right_inds = primitive._clf.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "## Plot decision scores\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(X_test)), y=pred_score, mode='lines', name='anomaly_score'))\n",
    "fig.update_layout(height=400, width=800, showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = pred_score.mean()\n",
    "sigma = pred_score.std()\n",
    "threshold = mu + 3*sigma\n",
    "y_pred = pred_score > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.DataFrame(X_test)\n",
    "pred_anomalies = pd.DataFrame(x_test.loc[np.where(y_pred == 1)])\n",
    "\n",
    "col = 0\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_test.index, y=x_test[col], mode='lines', name='x_test'))\n",
    "fig.add_trace(go.Scatter(x=pred_anomalies.index, y=pred_anomalies[col], mode='markers', name='Anomaly'))\n",
    "fig.update_layout(showlegend=True, xaxis_title=\"Time\", yaxis_title=\"value\", height=400, width=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_anomalies = pd.DataFrame(x_test.iloc[np.where(y_true == 1)])\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=x_test.index, y=x_test[col], mode='lines', name='x_test'))\n",
    "fig.add_trace(go.Scatter(x=real_anomalies.index, y=real_anomalies[col], mode='markers', name='Anomaly'))\n",
    "fig.update_layout(showlegend=True, xaxis_title=\"Time\", yaxis_title=\"value\", height=400, width=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "f1_scores = 2*recall*precision/(recall+precision)\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Score: ', accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best threshold: ', thresholds[np.argmax(f1_scores)])\n",
    "print('Best F1-Score: ', np.max(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('ROC')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results2\n",
    "results['timestamp'] = X_test.index[:results.shape[0]]"
   ]
  },
  {
   "source": [
    "## Step 4. Label Anomalies on the Test Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Step 5. Evaluate Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltr.plot_anomaly(match_data, y.loc[results['timestamp']], col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "ac = accuracy_score(results['label'], y.loc[results['timestamp']])\n",
    "tn, fp, fn, tp  = confusion_matrix(results['label'], y.loc[results['timestamp']]['label']).ravel()\n",
    "\n",
    "print('Accurancy:', ac)\n",
    "print('TN:', tn)\n",
    "print('FP:', fp)\n",
    "print('FN:', fn)\n",
    "print('TP:', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.14 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.14"
  },
  "interpreter": {
   "hash": "30295c5bec572e859485b1ffa5e89b8b3e2022ef6e3e739c1ac40f143a557caf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}